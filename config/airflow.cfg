[core]
airflow_home = /opt/airflow
dags_folder =  /opt/airflow/dags #s3://etl-airflow-alejandro/dags/
remote_logging = False
executor = LocalExecutor
parallelism = 32
default_timezone = utc


[webserver]
# The base URL of your Airflow instance
base_url = http://localhost:8080

# The webserver's IP address
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Turn off authentication (not recommended for production)
#authenticate = False
rbac = True
# Expose the configuration in the web UI
expose_config = True

[database]
# Connection string to your metadata database
sql_alchemy_conn = sqlite:////opt/airflow/airflow.db

[scheduler]
# The number of seconds to wait between scheduling iterations
job_heartbeat_sec = 5

# The number of seconds to wait between polling the executor for completed tasks
scheduler_heartbeat_sec = 5

[logging]
# The folder where logs are stored
base_log_folder = /opt/airflow/logs

# The logging level
logging_level = INFO

[celery]
# Configuration for CeleryExecutor (if you decide to use it)
broker_url = redis://redis:6379/0
result_backend = db+sqlite:///airflow.db

[secrets]
# Backend for storing Airflow variables and connections
backend = airflow.secrets.local_filesystem.LocalFilesystemBackend
#backend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend
#backend_kwargs = {"connections_prefix": "/airflow/connections", "variables_prefix": "/airflow/variables"}

[metrics]
# Enable StatsD metrics
statsd_on = True
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow
