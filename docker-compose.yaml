# Configuración básica de un clúster de Airflow con CeleryExecutor utilizando Redis y PostgreSQL.
# Este docker-compose.yaml configura un entorno distribuido de Airflow,
# donde las tareas se ejecutan de forma asíncrona mediante CeleryExecutor,
# utilizando Redis como backend de mensajes y PostgreSQL como base de datos.
# Lista de servicios e imágenes creadas:
# Servicios:
# - **postgres** (imagen: postgres:13): Base de datos PostgreSQL utilizada por Airflow para almacenar metadatos y estados de tareas.
# - **redis** (imagen: redis:7.2-bookworm): Servidor Redis que actúa como broker de mensajes para Celery.
# - **airflow-webserver** (imagen: apache/airflow:2.10.3): Interfaz web de Airflow para monitorear y administrar DAGs y tareas.
# - **airflow-scheduler** (imagen: apache/airflow:2.10.3): Planificador de Airflow que programa y desencadena tareas según los DAGs.
# - **airflow-worker** (imagen: apache/airflow:2.10.3): Trabajadores de Celery que ejecutan las tareas asignadas por el planificador.
# - **airflow-triggerer** (imagen: apache/airflow:2.10.3): Servicio que maneja disparadores asíncronos en Airflow.
# - **airflow-init** (imagen: apache/airflow:2.10.3): Servicio de inicialización que configura Airflow antes de iniciar otros servicios.
# - **airflow-cli** (imagen: apache/airflow:2.10.3): Herramienta de línea de comandos para interactuar con Airflow (perfil de depuración).
# - **flower** (imagen: apache/airflow:2.10.3): Interfaz web para monitorear y administrar los trabajadores de Celery (opcional).

#
# Esta configuración admite una configuración básica utilizando variables de entorno o un archivo .env.
# Las siguientes variables son soportadas:
#
# AIRFLOW_IMAGE_NAME           - Nombre de la imagen Docker utilizada para ejecutar Airflow.
#                                Valor predeterminado: apache/airflow:2.10.3
# AIRFLOW_UID                  - ID de usuario en los contenedores de Airflow.
#                                Valor predeterminado: 50000
# AIRFLOW_PROJ_DIR             - Ruta base a la que se montarán todos los archivos.
#                                Valor predeterminado: .
# Estas configuraciones son útiles principalmente en casos de pruebas individuales o ejecución de Airflow en modo de prueba.
#
# _AIRFLOW_WWW_USER_USERNAME   - Nombre de usuario para la cuenta de administrador (si se solicita).
#                                Valor predeterminado: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Contraseña para la cuenta de administrador (si se solicita).
#                                Valor predeterminado: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Requisitos adicionales de PIP para agregar al iniciar todos los contenedores.
#                                Utilice esta opción SOLO para comprobaciones rápidas. Instalar requisitos al inicio del contenedor
#                                se realiza CADA VEZ que se inicia el servicio, lo cual puede ser ineficiente.
#                                Una mejor práctica es construir una imagen personalizada o extender la imagen oficial
#                                como se describe en https://airflow.apache.org/docs/docker-stack/build.html.
#                                Valor predeterminado: ''
#
---
x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.3}
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
    AIRFLOW__CORE__STORE_SERIALIZED_DAGS: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE: 64
    AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW: 64
    AIRFLOW__SCHEDULER__MAX_THREADS: 64
    AIRFLOW__SCHEDULER__MAX_DAGRUNS_PER_LOOP_TO_SCHEDULE: 128
    AIRFLOW__SCHEDULER__PARSER_PROCESSING_PLANTS: 64
    AIRFLOW__CORE__PARALLELISM: 128
    AIRFLOW__CORE__DAG_CONCURRENCY: 64
    AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 64
    AIRFLOW__CELERY__WORKER_CONCURRENCY: 128
    AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 64
    AIRFLOW__WEBSERVER__WORKERS: 8
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: 10
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 100
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: 10
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_FETCH_INTERVAL: 10
    # yamllint disable rule:line-length
    # Utilice un servidor HTTP simple en el scheduler para comprobaciones de estado
    # Consulte https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # ADVERTENCIA: Utilice la opción _PIP_ADDITIONAL_REQUIREMENTS SOLO para comprobaciones rápidas.
    # Para otros propósitos (desarrollo, pruebas y especialmente uso en producción), construya o extienda la imagen de Airflow.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # La siguiente línea puede usarse para establecer un archivo de configuración personalizado, almacenado en la carpeta local de configuración.
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  redis:
    # Redis está limitado a 7.2-bookworm debido al cambio de licencia
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Requerido para manejar adecuadamente el apagado suave de los trabajadores de Celery
      # Consulte https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mADVERTENCIA!!!: AIRFLOW_UID no está establecido!\e[0m"
          echo "Si está en Linux, DEBE seguir las instrucciones a continuación para establecer "
          echo "la variable de entorno AIRFLOW_UID, de lo contrario los archivos serán propiedad de root."
          echo "Para otros sistemas operativos, puede eliminar la advertencia creando manualmente un archivo .env:"
          echo "    Ver: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mADVERTENCIA!!!: No hay suficiente memoria disponible para Docker.\e[0m"
          echo "Se requieren al menos 4GB de memoria. Usted tiene $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mADVERTENCIA!!!: No hay suficientes CPU disponibles para Docker.\e[0m"
          echo "Se recomiendan al menos 2 CPU. Usted tiene $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mADVERTENCIA!!!: No hay suficiente espacio en disco disponible para Docker.\e[0m"
          echo "Se recomiendan al menos 10 GB. Usted tiene $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mADVERTENCIA!!!: ¡No tiene suficientes recursos para ejecutar Airflow (ver arriba)!\e[0m"
          echo "Por favor, siga las instrucciones para aumentar la cantidad de recursos disponibles:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi

        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}

        airflow connections delete 'aws_default' || true
        airflow connections add 'aws_default' \
          --conn-type 'aws' \
          --conn-login '${AWS_ACCESS_KEY_ID:-xxx}' \
          --conn-password '${AWS_SECRET_ACCESS_KEY:-xxx}' \
          --conn-extra "{\"region_name\": \"${AWS_DEFAULT_REGION:-us-east-1}\"}"

        airflow pools set extract_pool 8 "pool para tareas de extracción"
        airflow pools set transform_pool 8 "pool para tareas de transformación"
        airflow pools set load_pool 8 "pool para tareas de carga"

        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Solución temporal para el problema del entrypoint. Ver: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # Puede habilitar Flower agregando la opción "--profile flower", por ejemplo, `docker-compose --profile flower up`,
  # o apuntando explícitamente en la línea de comandos, por ejemplo, `docker-compose up flower`.
  # Ver: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:
